OPENMP LAB NOTEBOOK
--------------
/*
 * NOTE: I wrote my steps in a procedural manner so that I can
 * see clearly what I did and can reproduce the results later.
 */
What I did:

Step 0: as explained in TA office hours, the program must be run in lnxsrv09
not lnxsrv07

1. Download the source code from ccle
2. Unzip the tar.gz file and drag it into the SEASNET machine
3. navigate into the openmplab/ folder
4. Analyze for the bottle neck:
	Commands:
	$ make seq
		creates an executable that displays the execution time
	$ ./seq
		runs the executable
		OUTPUT:
			FUNC TIME: 0.482636
			TOTAL TIME: 2.249850
	$ make seq GPROF=1
		gprof gives the time for each function, so that we can
		find the bottleneck 
	$ gprof seq | less
		OUTPUT:
			% time	. . .	name
			59.40	. . .	func1
			20.37	. . .	rand2
			8.49	. . .	addSeed
			3.39	. . .	findIndexBin
			3.39	. . .	sequence
			1.70	. . .	func3
			1.70	. . .	func5
			1.70	. . .	imdilateDisk
			0.00	. . .	round
			etc.
		NOTE: We can see that the bottle neck is func1
5. Locate the file func.c and begin performing optimizations:
	Optimizations are subject to more creativity. I first, optimized
	func1, and reached the target 350% speed improvement. I later
	optimized the other functions for even better performance. The
	specific optimizations techniques I used were: thread-level
	parallelism, memory-hierarchy, and loop unrolling. Specific number
	of threads to create and where to optimize have many factors
	(we don't want too many threads nor do we want too little threads)
	NOTE: seasnet machines have a cap to there thread limit. The number
	of threads I used was 50 as I found that number gave good performance
	throughout. More in-depth optimizations can be found below in the
	"Copy of func.c" Section.
6. After making optimizations to func.c, test the program speed-up:
	NOTE: it is advised to develop incrementally and to try testing
	after each optimization done.
	Commands:
	$ make omp
		similar to seq but allows us to use openmp and thread-level
		parallelism
	$ ./omp
		runs the executable 
		OUTPUT:
			FUNC TIME: 0.052093
			TOTAL TIME: 1.991830
		NOTE: This is after I have done all of my optimizations.
		Additionally, every run gives me different number just based
		on how the threads are run by the cpu. I usually attain a func
		time of a little < 0.053. Sometimes, it goes as high as 0.054 and
		low as 0.052. The speedup of this trial is 926.5%.
		NOTE: I tried to run omp again a while later, and this number
		altered to around ~0.054 this is probably due to more people on
		the linux server doing this lab, causing my program to execute
		more slowly. 
		NOTE: there are rare cases when the program runs slower than 0.060 or
		faster than 0.051, but I treated these as off-cases since all of my
		thread numbers are defined. Most likely, again, this on the linux
		server's end.
	$ make check
		checks if the program still does what it should
		OUTPUT:
			gcc -o omp  -O3 -fopenmp filter.c main.c func.c util.c -lm
			cp omp filter
			./filter
			FUNC TIME : 0.052404
			TOTAL TIME : 1.940230
			diff --brief correct.txt output.txt
		NOTE: This means there is no error. If there were errors, the
		terminal will print Error 1: . . .
	$ make omp MTRACE=1
		this is used to check for memory leaks
	$ ./omp
	$ make checkmem
		OUTPUT:
 			mtrace filter mtrace.out || true

			Memory not freed:
			-----------------
			           Address     Size     Caller
			0x0000000001076070   0x2f70  at 0x7fa56dab6869
			0x0000000001078ff0     0xc0  at 0x7fa56dab6869
			0x00000000010790c0    0x198  at 0x7fa56dab68b9
			0x0000000001079260    0x240  at 0x7fa56dfe6c25
			0x00000000010794b0    0x240  at 0x7fa56dfe6c25
			0x0000000001079700    0x240  at 0x7fa56dfe6c25
			0x0000000001079950    0x240  at 0x7fa56dfe6c25
			0x0000000001079ba0    0x240  at 0x7fa56dfe6c25
			0x0000000001079df0    0x240  at 0x7fa56dfe6c25
			0x000000000107a040    0x240  at 0x7fa56dfe6c25
			0x000000000107a290    0x240  at 0x7fa56dfe6c25
			0x000000000107a4e0    0x240  at 0x7fa56dfe6c25
			0x000000000107a730    0x240  at 0x7fa56dfe6c25
			0x000000000107a980    0x240  at 0x7fa56dfe6c25
			0x000000000107abd0    0x240  at 0x7fa56dfe6c25
			0x000000000107ae20    0x240  at 0x7fa56dfe6c25
			0x000000000107b070    0x240  at 0x7fa56dfe6c25
			0x000000000107b2c0    0x240  at 0x7fa56dfe6c25
			0x000000000107b510    0x240  at 0x7fa56dfe6c25
			0x000000000107b760    0x240  at 0x7fa56dfe6c25
			etc...
		NOTE: After asking TA Jin Wang, it is believed that these
		memory leaks do not have anything to do with the parallelism
		I've implemented (I never called malloc()) and can be 
		disregarded. After office hours on 06/08/17, it is determined
		that the memory leaks relate with the way OPENMP is implemented.
7. Repeat steps 5. and 6. until satisfied with the performance speedup.
8. Conclusion
	The speedup of the program can be found by dividing the final speed
	with the initial speed, as described above, my optimized code reached
	a speedup of >900%.
		
COPY OF FUNC.C
--------------
/*
 * README:
 * I included tags to each of the individual optimizations I performed in a comment
 * below the specified optimization. The description for each of the optimizations
 * can be found in the legend at the bottom of this section. Some lines of the code
 * had been truncated to meet the 200 char limit. Again, the original func.c file
 * was also submitted. 
 */

#include "func.h"
#include "util.h"

void func0(double *weights, double *arrayX, double *arrayY, int xr, int yr, int n)
{
	int i;
	#pragma omp parallel for num_threads(50) firstprivate(weights, arrayX, arrayY) private(i)
	//OPTIMIZATION 1
	for(i = 0; i < n; ++i){
	//OPTIMIZATION 2
		weights[i] = 1/((double)(n));
		arrayX[i] = xr;
		arrayY[i] = yr;
	}
}

void func1(int *seed, int *array, double *arrayX, double *arrayY,
			double *probability, double *objxy, int *index,
			int Ones, int iter, int X, int Y, int Z, int n)
{
	int i, j;
   	int index_X, index_Y;
	int max_size = X*Y*Z;

	omp_set_num_threads(50);
	//OPTIMIZATION 3
	#pragma omp parallel for firstprivate(arrayX, arrayY) private (i)
	//OPTIMIZATION 4
   	for(i = 0; i < n; ++i){
	//OPTIMIZATION 2
   		arrayX[i] += 1 + 5*rand2(seed, i);
   		arrayY[i] += -2 + 2*rand2(seed, i);
   	}

	#pragma omp parallel for firstprivate(index_X, index_Y, index, probability) private(i, j)
	//OPTIMiZATION 4
   	for(i = 0; i<n; ++i){
	//OPTIMIZATION 2
		int temp = i*Ones;
	//OPTIMIZATION 5
   		for(j = 0; j < Ones; ++j){
	//OPTIMIZATION 2
   			index_X = round(arrayX[i]) + objxy[j + j + 1];
   			index_Y = round(arrayY[i]) + objxy[j + j];
	//OPTIMIZATION 6 (both lines above)
   			index[temp + j] = fabs(index_X*Y*Z + index_Y*Z + iter);
   			if(index[temp + j] >= max_size)
   				index[temp + j] = 0;
	//OPTIMIZATION 5 (3 lines above)
   		}
   		probability[i] = 0;

   		for(j = 0; j < Ones; ++j) {
	//OPTIMIZATION 2
   			probability[i] += (pow((array[index[temp + j]] - 100),2) -
					  pow((array[index[temp + j]]-228),2))/50.0;
	//OPTIMIZATION 5
   		}
   		probability[i] = probability[i]/((double) Ones);
   	}
}

void func2(double *weights, double *probability, int n)
{
	int i;
	double sumWeights=0;

	omp_set_num_threads(50);
	//OPTIMIZATION 3
	#pragma omp parallel for firstprivate(weights) private(i) 
	//OPTIMIZATION 4
	for(i = 0; i < n; ++i)
	//OPTIMIZATION 2
   		weights[i] = weights[i] * exp(probability[i]);

   	for(i = 0; i < n; ++i)
	//OPTIMIZATION 2
   		sumWeights += weights[i];

	#pragma omp parallel for firstprivate(sumWeights, weights) private(i) 
	//OPTIMIZATION 4
	for(i = 0; i < n; ++i)
	//OPTIMIZATION 2
   		weights[i] = weights[i]/sumWeights;
}

void func3(double *arrayX, double *arrayY, double *weights, double *x_e, double *y_e, int n)
{
	double estimate_x=0.0;
	double estimate_y=0.0;
    	int i;

	//NOTE: theoretically I can use OPTIMIZATION 1 here; however, testing this,
	//it made my program perform slower. This may be because the value for n
	//is not big enough for 50 threads, or just that the linux server was being
	//used extensively while I was testing it. Either way, the performance boost
	//of using OPTIMIZATION 1 in FUNC3 would be minimal anyways, as seen in gprof.
	for(i = 0; i < n; ++i){
	//OPTIMIZATION 2
   		estimate_x += arrayX[i] * weights[i];
   		estimate_y += arrayY[i] * weights[i];
   	}

	*x_e = estimate_x;
	*y_e = estimate_y;
}

void func4(double *u, double u1, int n)
{
	int i;

	#pragma omp parallel for num_threads(50) firstprivate(u, u1) private(i) 
	//OPTIMIZATION 1
	for(i = 0; i < n; ++i){
	//OPTIMIZATION 2
   		u[i] = u1 + i/((double)(n));
   	}
}

void func5(double *x_j, double *y_j, double *arrayX, double *arrayY, double *weights, ... 
{
	int i, j;

	omp_set_num_threads(50);
	//OPTIMIZATION 3
	#pragma omp parallel for firstprivate(x_j, y_j, i) private(j) 
	//OPTIMIZATION 4
	for(j = 0; j < n; ++j){
	//OPTIMIZATION 2
   		i = findIndexBin(cfd, 0, n, u[j]);
   		if(i == -1)
   			i = n-1;
   		x_j[j] = arrayX[i];
   		y_j[j] = arrayY[i];
   	}

	#pragma omp parallel for firstprivate(arrayX, arrayY, weights) private(i) 
	//OPTIMIZATION 4
	for(i = 0; i < n; ++i){
	//OPTIMIZATION 2
		arrayX[i] = x_j[i];
		arrayY[i] = y_j[i];
		weights[i] = 1/((double)(n));
	}
}


Explanations of each OPTIMIZATION:
OPTIMIZATION 1 - parallelize the for loop at a thread-level. This uses openmp to easily
		 create and join threads. The join is set after the for loop. to make the
		 join occur later, one would need to use brackets { } after the #pragma...
		 I call private on the iterators because it needs to be private between
		 threads and does not need to retain the vlaue before the parallelization.
		 Note, this is OK as the iterator get initialized in the for loop (ex/
		 for (i = 0; i < n; ++i). I call firstprivate on variables that get altered 
		 in the loop because they need to both be private between threads and retain
		 their values before parallelism. I chose the thread_num(50) through testing.
		 We cannot have too many threads that the cpu may not be designed to handel
		 it (SEASNET does not allow me to create more than ~100). We cannot have
		 too little # of threads because that limits throughput. 50 seemed to work
		 well for me, so I used 50. 
OPTIMIZATION 2 - ++i is faster than i++ for iterators, as ++i does not need to store values,
		 which saves us an instruction.  
OPTIMIZATION 3 - Similar to OPTIMIZATION 1. For functions with multiple for loops that need 
		 to be parallelized, it is more efficient to just declare the number of 
		 threads to be used before calling the #pragma call that parallelizes them.
		 This is substitute to writing thread_num(X) everytime #pragma is called. 
		 The number of threads was determined in the same way as OPTIMIZATION 1.
OPTIMIZATION 4 - Similar to OPTIMIZATION 1. Requires OPTIMIZATION 3 for stability in time
		 readings. This does the same thing as OPTIMIZATION 1, using the default
		 number of threads defined by OPTIMIZATION 3. Again, the number of threads
		 was explained in OPTIMIZATION 1 and OPTIMIZATION 3. 
OPTIMIZATION 5 - Sometimes the same value is calculated multiple times through a for loop.
		 I minimized the nunmber of instruction calls by performing the operation
		 once and saving it to memory. This also works because there is enough space
		 in the cache, such that cost of accessing that value in memory/cache
		 outweighs the continue calculations. This is especially effective for
		 repeated multiplications.
OPTIMIZATION 6 - Multiplication is a more expensive operation than addition in the machine
		 code by quite a lot. so when I can add two number instead of *2, I did so.
		 This did not change output.txt as checked with $make check. 

ADDITIONAL NOTES: 
For a 350% boost in speed all that needs to be optimized is func1. As shown by GRPOF, func1
is the bottleneck of the program. One can find the next bottle neck after optimizing func1
with '$make omp GPROF=1' but I chose to just optimize the rest of the functions as their
are not that many. Most of the speed boost comes from thread level parallelism as implemented
by OPENMP (hence the name of the lab), however OPTIMIZATION 5 and 6 provided large boosts
in speed as well. I experimented with other optimization techniques such as loop unrolling
but the performance boost was not noticable and the potential for bugs are larger, so I
did not decide to include it (the first several times I implemented this I got an error from
$make check, and though it eventaully worked, I would rather prefer correctness over 
performance in this case). Furthermore, the current optimizations have made the program run
quite fast >900%, so more optimizations may not be able to boost the speed that much more.
Unless we try really complicated optimizations (such as those used in the Intel C compilers)
beyond the scope of the class, I believe my current optimizations are reasonable.
		
